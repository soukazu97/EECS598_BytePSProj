gl1014
============>Torch version:  1.8.0
INFO:root:Worker rank 2 initializing RPC
INFO:root:Worker 2 done initializing RPC
INFO:root:Using 1 GPUs to train
INFO:root:Putting first 2 convs on cuda:0
INFO:root:Putting rest of layers on cuda:0
INFO:root:Number of batchs 938
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 0 loss 2.305522918701172
INFO:root:Rank 2 training batch 5 loss 2.2913568019866943
INFO:root:Rank 2 training batch 10 loss 2.2820568084716797
INFO:root:Rank 2 training batch 15 loss 2.232712984085083
INFO:root:Rank 2 training batch 20 loss 2.1930551528930664
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 25 loss 2.0917086601257324
INFO:root:Rank 2 training batch 30 loss 1.98171865940094
INFO:root:Rank 2 training batch 35 loss 1.7644821405410767
INFO:root:Rank 2 training batch 40 loss 1.4495569467544556
INFO:root:Rank 2 training batch 45 loss 1.3880974054336548
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 50 loss 1.162877082824707
INFO:root:Rank 2 training batch 55 loss 1.0999823808670044
INFO:root:Rank 2 training batch 60 loss 1.2131048440933228
INFO:root:Rank 2 training batch 65 loss 0.824963390827179
INFO:root:Rank 2 training batch 70 loss 1.2094019651412964
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 75 loss 0.8625971674919128
INFO:root:Rank 2 training batch 80 loss 0.9269152283668518
INFO:root:Rank 2 training batch 85 loss 0.9508783221244812
INFO:root:Rank 2 training batch 90 loss 0.8364489078521729
INFO:root:Rank 2 training batch 95 loss 0.7357363700866699
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 100 loss 0.8355106711387634
INFO:root:Rank 2 training batch 105 loss 0.5078384876251221
INFO:root:Rank 2 training batch 110 loss 0.5348122715950012
INFO:root:Rank 2 training batch 115 loss 0.7354286313056946
INFO:root:Rank 2 training batch 120 loss 0.5914507508277893
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 125 loss 0.48494255542755127
INFO:root:Rank 2 training batch 130 loss 0.3986937701702118
INFO:root:Rank 2 training batch 135 loss 1.0024851560592651
INFO:root:Rank 2 training batch 140 loss 0.741967499256134
INFO:root:Rank 2 training batch 145 loss 0.5762730240821838
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 150 loss 0.5951471924781799
INFO:root:Rank 2 training batch 155 loss 0.636529803276062
INFO:root:Rank 2 training batch 160 loss 0.4130818545818329
INFO:root:Rank 2 training batch 165 loss 0.7498287558555603
INFO:root:Rank 2 training batch 170 loss 0.632225513458252
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 175 loss 0.3898128271102905
INFO:root:Rank 2 training batch 180 loss 0.39385461807250977
INFO:root:Rank 2 training batch 185 loss 0.6866016983985901
INFO:root:Rank 2 training batch 190 loss 0.381694495677948
INFO:root:Rank 2 training batch 195 loss 0.5061246156692505
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 200 loss 0.8248294591903687
INFO:root:Rank 2 training batch 205 loss 0.9426552653312683
INFO:root:Rank 2 training batch 210 loss 0.6942986249923706
INFO:root:Rank 2 training batch 215 loss 0.4383186101913452
INFO:root:Rank 2 training batch 220 loss 0.6270174980163574
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 225 loss 0.6890192627906799
INFO:root:Rank 2 training batch 230 loss 0.4893314838409424
INFO:root:Rank 2 training batch 235 loss 0.30830246210098267
INFO:root:Rank 2 training batch 240 loss 0.60249263048172
INFO:root:Rank 2 training batch 245 loss 0.6114063858985901
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 250 loss 0.4212256371974945
INFO:root:Rank 2 training batch 255 loss 0.9578753113746643
INFO:root:Rank 2 training batch 260 loss 0.5583415031433105
INFO:root:Rank 2 training batch 265 loss 0.5852465629577637
INFO:root:Rank 2 training batch 270 loss 0.4351612627506256
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 275 loss 0.42322802543640137
INFO:root:Rank 2 training batch 280 loss 0.2686082720756531
INFO:root:Rank 2 training batch 285 loss 0.539384663105011
INFO:root:Rank 2 training batch 290 loss 0.6354278922080994
INFO:root:Rank 2 training batch 295 loss 0.4102320373058319
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 300 loss 0.4115169048309326
INFO:root:Rank 2 training batch 305 loss 0.2441686987876892
INFO:root:Rank 2 training batch 310 loss 0.697109043598175
INFO:root:Rank 2 training batch 315 loss 0.2814878523349762
INFO:root:Rank 2 training batch 320 loss 0.7210917472839355
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 325 loss 0.5793284177780151
INFO:root:Rank 2 training batch 330 loss 0.4248064458370209
INFO:root:Rank 2 training batch 335 loss 0.3968053460121155
INFO:root:Rank 2 training batch 340 loss 0.46105897426605225
INFO:root:Rank 2 training batch 345 loss 0.7042049169540405
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 350 loss 0.35448548197746277
INFO:root:Rank 2 training batch 355 loss 0.37358948588371277
INFO:root:Rank 2 training batch 360 loss 0.38765591382980347
INFO:root:Rank 2 training batch 365 loss 0.38315117359161377
INFO:root:Rank 2 training batch 370 loss 0.5013006329536438
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 375 loss 0.7496508359909058
INFO:root:Rank 2 training batch 380 loss 0.2939314544200897
INFO:root:Rank 2 training batch 385 loss 0.5465551018714905
INFO:root:Rank 2 training batch 390 loss 0.21106740832328796
INFO:root:Rank 2 training batch 395 loss 0.7098218202590942
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 400 loss 0.21485839784145355
INFO:root:Rank 2 training batch 405 loss 0.19038479030132294
INFO:root:Rank 2 training batch 410 loss 0.4368361234664917
INFO:root:Rank 2 training batch 415 loss 0.229961559176445
INFO:root:Rank 2 training batch 420 loss 0.5050520896911621
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 425 loss 0.4488801956176758
INFO:root:Rank 2 training batch 430 loss 0.2648080587387085
INFO:root:Rank 2 training batch 435 loss 0.4199697971343994
INFO:root:Rank 2 training batch 440 loss 0.564170777797699
INFO:root:Rank 2 training batch 445 loss 0.30696359276771545
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 450 loss 0.19181355834007263
INFO:root:Rank 2 training batch 455 loss 0.5361018180847168
INFO:root:Rank 2 training batch 460 loss 0.23509976267814636
INFO:root:Rank 2 training batch 465 loss 0.3844761848449707
INFO:root:Rank 2 training batch 470 loss 0.47392645478248596
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 475 loss 0.16611185669898987
INFO:root:Rank 2 training batch 480 loss 0.24829113483428955
INFO:root:Rank 2 training batch 485 loss 0.20873495936393738
INFO:root:Rank 2 training batch 490 loss 0.11178165674209595
INFO:root:Rank 2 training batch 495 loss 0.3692375421524048
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 500 loss 0.31186792254447937
INFO:root:Rank 2 training batch 505 loss 0.5570341944694519
INFO:root:Rank 2 training batch 510 loss 0.4605548679828644
INFO:root:Rank 2 training batch 515 loss 0.40966275334358215
INFO:root:Rank 2 training batch 520 loss 0.3313131630420685
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 525 loss 0.4297995865345001
INFO:root:Rank 2 training batch 530 loss 0.19699259102344513
INFO:root:Rank 2 training batch 535 loss 0.20754604041576385
INFO:root:Rank 2 training batch 540 loss 0.2252655178308487
INFO:root:Rank 2 training batch 545 loss 0.08532550185918808
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 550 loss 0.381719172000885
INFO:root:Rank 2 training batch 555 loss 0.2588621973991394
INFO:root:Rank 2 training batch 560 loss 0.13301308453083038
INFO:root:Rank 2 training batch 565 loss 0.3078761696815491
INFO:root:Rank 2 training batch 570 loss 0.28851205110549927
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 575 loss 0.2216285616159439
INFO:root:Rank 2 training batch 580 loss 0.33009767532348633
INFO:root:Rank 2 training batch 585 loss 0.24622538685798645
INFO:root:Rank 2 training batch 590 loss 0.3170778453350067
INFO:root:Rank 2 training batch 595 loss 0.33607932925224304
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 600 loss 0.285769522190094
INFO:root:Rank 2 training batch 605 loss 0.20386047661304474
INFO:root:Rank 2 training batch 610 loss 0.11053171753883362
INFO:root:Rank 2 training batch 615 loss 0.13853590190410614
INFO:root:Rank 2 training batch 620 loss 0.1664934754371643
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 625 loss 0.262021541595459
INFO:root:Rank 2 training batch 630 loss 0.9118881821632385
INFO:root:Rank 2 training batch 635 loss 0.32007476687431335
INFO:root:Rank 2 training batch 640 loss 0.27682051062583923
INFO:root:Rank 2 training batch 645 loss 0.26570916175842285
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 650 loss 0.7077368497848511
INFO:root:Rank 2 training batch 655 loss 0.2592584490776062
INFO:root:Rank 2 training batch 660 loss 0.14657585322856903
INFO:root:Rank 2 training batch 665 loss 0.1986979991197586
INFO:root:Rank 2 training batch 670 loss 0.21906723082065582
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 675 loss 0.3966396450996399
INFO:root:Rank 2 training batch 680 loss 0.35042643547058105
INFO:root:Rank 2 training batch 685 loss 0.23581568896770477
INFO:root:Rank 2 training batch 690 loss 0.1677626073360443
INFO:root:Rank 2 training batch 695 loss 0.1715950071811676
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 700 loss 0.21852803230285645
INFO:root:Rank 2 training batch 705 loss 0.2902018427848816
INFO:root:Rank 2 training batch 710 loss 0.3332657516002655
INFO:root:Rank 2 training batch 715 loss 0.5203802585601807
INFO:root:Rank 2 training batch 720 loss 0.2823474705219269
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 725 loss 0.2785397171974182
INFO:root:Rank 2 training batch 730 loss 0.14680351316928864
INFO:root:Rank 2 training batch 735 loss 0.3451888859272003
INFO:root:Rank 2 training batch 740 loss 0.09356463700532913
INFO:root:Rank 2 training batch 745 loss 0.3594616651535034
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 750 loss 0.32727187871932983
INFO:root:Rank 2 training batch 755 loss 0.2223518341779709
INFO:root:Rank 2 training batch 760 loss 0.3584859371185303
INFO:root:Rank 2 training batch 765 loss 0.17481252551078796
INFO:root:Rank 2 training batch 770 loss 0.1460157036781311
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 775 loss 0.2105902135372162
INFO:root:Rank 2 training batch 780 loss 0.41649094223976135
INFO:root:Rank 2 training batch 785 loss 0.24097661674022675
INFO:root:Rank 2 training batch 790 loss 0.20224273204803467
INFO:root:Rank 2 training batch 795 loss 0.20337747037410736
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 800 loss 0.32423388957977295
INFO:root:Rank 2 training batch 805 loss 0.2261733114719391
INFO:root:Rank 2 training batch 810 loss 0.14297927916049957
INFO:root:Rank 2 training batch 815 loss 0.3958476781845093
INFO:root:Rank 2 training batch 820 loss 0.12836581468582153
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 825 loss 0.32126757502555847
INFO:root:Rank 2 training batch 830 loss 0.17567932605743408
INFO:root:Rank 2 training batch 835 loss 0.27460867166519165
INFO:root:Rank 2 training batch 840 loss 0.23542341589927673
INFO:root:Rank 2 training batch 845 loss 0.3282497525215149
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 850 loss 0.2245543897151947
INFO:root:Rank 2 training batch 855 loss 0.21043206751346588
INFO:root:Rank 2 training batch 860 loss 0.20601430535316467
INFO:root:Rank 2 training batch 865 loss 0.4865680932998657
INFO:root:Rank 2 training batch 870 loss 0.5078045129776001
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 875 loss 0.07359715551137924
INFO:root:Rank 2 training batch 880 loss 0.46875014901161194
INFO:root:Rank 2 training batch 885 loss 0.42719000577926636
INFO:root:Rank 2 training batch 890 loss 0.23607689142227173
INFO:root:Rank 2 training batch 895 loss 0.17839668691158295
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 900 loss 0.233708918094635
INFO:root:Rank 2 training batch 905 loss 0.23723074793815613
INFO:root:Rank 2 training batch 910 loss 0.26332151889801025
INFO:root:Rank 2 training batch 915 loss 0.15694232285022736
INFO:root:Rank 2 training batch 920 loss 0.0483582578599453
INFO:root:Rank 2 averaging model across all trainers.
INFO:root:Rank 2 training batch 925 loss 0.20305077731609344
INFO:root:Rank 2 training batch 930 loss 0.23278658092021942
INFO:root:Rank 2 training batch 935 loss 0.21658597886562347
INFO:root:Training complete!
INFO:root:Getting accuracy....
INFO:root:Accuracy 0.9297
