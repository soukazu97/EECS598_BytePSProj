gl1011
============>Torch version:  1.8.0
INFO:root:Worker rank 1 initializing RPC
INFO:root:Worker 1 done initializing RPC
INFO:root:Using 1 GPUs to train
INFO:root:Putting first 2 convs on cuda:0
INFO:root:Putting rest of layers on cuda:0
INFO:root:Number of batchs 938
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 0 loss 2.3082287311553955
INFO:root:Rank 1 training batch 5 loss 2.2795324325561523
INFO:root:Rank 1 training batch 10 loss 2.240027666091919
INFO:root:Rank 1 training batch 15 loss 2.1968424320220947
INFO:root:Rank 1 training batch 20 loss 2.056871175765991
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 25 loss 2.0000057220458984
INFO:root:Rank 1 training batch 30 loss 1.7616709470748901
INFO:root:Rank 1 training batch 35 loss 1.4374390840530396
INFO:root:Rank 1 training batch 40 loss 1.2709314823150635
INFO:root:Rank 1 training batch 45 loss 1.3272554874420166
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 50 loss 0.9656842350959778
INFO:root:Rank 1 training batch 55 loss 1.1640307903289795
INFO:root:Rank 1 training batch 60 loss 1.048727035522461
INFO:root:Rank 1 training batch 65 loss 1.0673562288284302
INFO:root:Rank 1 training batch 70 loss 0.9132640361785889
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 75 loss 0.7199494242668152
INFO:root:Rank 1 training batch 80 loss 0.7487072944641113
INFO:root:Rank 1 training batch 85 loss 0.5827460289001465
INFO:root:Rank 1 training batch 90 loss 0.4870437979698181
INFO:root:Rank 1 training batch 95 loss 0.922817051410675
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 100 loss 1.0245534181594849
INFO:root:Rank 1 training batch 105 loss 0.5098728537559509
INFO:root:Rank 1 training batch 110 loss 0.6174428462982178
INFO:root:Rank 1 training batch 115 loss 1.082603931427002
INFO:root:Rank 1 training batch 120 loss 0.4455414116382599
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 125 loss 0.5338900089263916
INFO:root:Rank 1 training batch 130 loss 0.47238075733184814
INFO:root:Rank 1 training batch 135 loss 0.8961830139160156
INFO:root:Rank 1 training batch 140 loss 0.5480807423591614
INFO:root:Rank 1 training batch 145 loss 0.35136666893959045
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 150 loss 0.6765667200088501
INFO:root:Rank 1 training batch 155 loss 0.4989084303379059
INFO:root:Rank 1 training batch 160 loss 0.4733979105949402
INFO:root:Rank 1 training batch 165 loss 0.4514770805835724
INFO:root:Rank 1 training batch 170 loss 0.2714013159275055
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 175 loss 0.4988081753253937
INFO:root:Rank 1 training batch 180 loss 0.5073552131652832
INFO:root:Rank 1 training batch 185 loss 0.6752442121505737
INFO:root:Rank 1 training batch 190 loss 0.46496859192848206
INFO:root:Rank 1 training batch 195 loss 0.626454770565033
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 200 loss 0.37263932824134827
INFO:root:Rank 1 training batch 205 loss 1.1623505353927612
INFO:root:Rank 1 training batch 210 loss 0.5228868126869202
INFO:root:Rank 1 training batch 215 loss 0.7099282145500183
INFO:root:Rank 1 training batch 220 loss 0.2738191485404968
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 225 loss 0.16052956879138947
INFO:root:Rank 1 training batch 230 loss 0.447889119386673
INFO:root:Rank 1 training batch 235 loss 0.6055923104286194
INFO:root:Rank 1 training batch 240 loss 0.44058844447135925
INFO:root:Rank 1 training batch 245 loss 0.2732211649417877
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 250 loss 0.5097547173500061
INFO:root:Rank 1 training batch 255 loss 0.12585586309432983
INFO:root:Rank 1 training batch 260 loss 0.7639544606208801
INFO:root:Rank 1 training batch 265 loss 0.4023401737213135
INFO:root:Rank 1 training batch 270 loss 0.37281906604766846
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 275 loss 0.39967110753059387
INFO:root:Rank 1 training batch 280 loss 0.49148568511009216
INFO:root:Rank 1 training batch 285 loss 0.533420741558075
INFO:root:Rank 1 training batch 290 loss 0.5191361904144287
INFO:root:Rank 1 training batch 295 loss 0.5140552520751953
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 300 loss 0.49359679222106934
INFO:root:Rank 1 training batch 305 loss 0.4122796952724457
INFO:root:Rank 1 training batch 310 loss 0.8213776350021362
INFO:root:Rank 1 training batch 315 loss 0.41293561458587646
INFO:root:Rank 1 training batch 320 loss 0.6600826382637024
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 325 loss 0.30204084515571594
INFO:root:Rank 1 training batch 330 loss 0.5109019875526428
INFO:root:Rank 1 training batch 335 loss 0.4548491835594177
INFO:root:Rank 1 training batch 340 loss 0.18773432075977325
INFO:root:Rank 1 training batch 345 loss 0.48838019371032715
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 350 loss 0.6926262974739075
INFO:root:Rank 1 training batch 355 loss 0.3273996114730835
INFO:root:Rank 1 training batch 360 loss 0.5788609385490417
INFO:root:Rank 1 training batch 365 loss 0.34497880935668945
INFO:root:Rank 1 training batch 370 loss 0.4099199175834656
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 375 loss 0.3284917175769806
INFO:root:Rank 1 training batch 380 loss 0.3652745187282562
INFO:root:Rank 1 training batch 385 loss 0.5718328952789307
INFO:root:Rank 1 training batch 390 loss 0.47943222522735596
INFO:root:Rank 1 training batch 395 loss 0.5071548223495483
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 400 loss 0.3873125910758972
INFO:root:Rank 1 training batch 405 loss 0.20203891396522522
INFO:root:Rank 1 training batch 410 loss 0.5030929446220398
INFO:root:Rank 1 training batch 415 loss 0.24214360117912292
INFO:root:Rank 1 training batch 420 loss 0.3975430727005005
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 425 loss 0.5258176326751709
INFO:root:Rank 1 training batch 430 loss 0.254851371049881
INFO:root:Rank 1 training batch 435 loss 0.22770865261554718
INFO:root:Rank 1 training batch 440 loss 0.820605456829071
INFO:root:Rank 1 training batch 445 loss 0.43014585971832275
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 450 loss 0.5508227348327637
INFO:root:Rank 1 training batch 455 loss 0.23480768501758575
INFO:root:Rank 1 training batch 460 loss 0.641396164894104
INFO:root:Rank 1 training batch 465 loss 0.22206443548202515
INFO:root:Rank 1 training batch 470 loss 0.23230773210525513
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 475 loss 0.24612043797969818
INFO:root:Rank 1 training batch 480 loss 0.24041149020195007
INFO:root:Rank 1 training batch 485 loss 0.21259649097919464
INFO:root:Rank 1 training batch 490 loss 0.31606125831604004
INFO:root:Rank 1 training batch 495 loss 0.31384357810020447
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 500 loss 0.21581301093101501
INFO:root:Rank 1 training batch 505 loss 0.4139232635498047
INFO:root:Rank 1 training batch 510 loss 0.18679706752300262
INFO:root:Rank 1 training batch 515 loss 0.20845948159694672
INFO:root:Rank 1 training batch 520 loss 0.4043165445327759
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 525 loss 0.29287245869636536
INFO:root:Rank 1 training batch 530 loss 0.11648042500019073
INFO:root:Rank 1 training batch 535 loss 0.41886478662490845
INFO:root:Rank 1 training batch 540 loss 0.6031829714775085
INFO:root:Rank 1 training batch 545 loss 0.18117624521255493
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 550 loss 0.23030470311641693
INFO:root:Rank 1 training batch 555 loss 0.23361849784851074
INFO:root:Rank 1 training batch 560 loss 0.518980860710144
INFO:root:Rank 1 training batch 565 loss 0.3720614016056061
INFO:root:Rank 1 training batch 570 loss 0.7132959961891174
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 575 loss 0.22070300579071045
INFO:root:Rank 1 training batch 580 loss 0.4856365919113159
INFO:root:Rank 1 training batch 585 loss 0.2622022330760956
INFO:root:Rank 1 training batch 590 loss 0.19755740463733673
INFO:root:Rank 1 training batch 595 loss 0.4201575517654419
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 600 loss 0.18419718742370605
INFO:root:Rank 1 training batch 605 loss 0.215787872672081
INFO:root:Rank 1 training batch 610 loss 0.27402618527412415
INFO:root:Rank 1 training batch 615 loss 0.34091654419898987
INFO:root:Rank 1 training batch 620 loss 0.20202593505382538
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 625 loss 0.4473956823348999
INFO:root:Rank 1 training batch 630 loss 0.33701789379119873
INFO:root:Rank 1 training batch 635 loss 0.5219395756721497
INFO:root:Rank 1 training batch 640 loss 0.12467890232801437
INFO:root:Rank 1 training batch 645 loss 0.5107623338699341
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 650 loss 0.2949625551700592
INFO:root:Rank 1 training batch 655 loss 0.09539765119552612
INFO:root:Rank 1 training batch 660 loss 0.2758018374443054
INFO:root:Rank 1 training batch 665 loss 0.23436881601810455
INFO:root:Rank 1 training batch 670 loss 0.3437260389328003
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 675 loss 0.5595525503158569
INFO:root:Rank 1 training batch 680 loss 0.21279187500476837
INFO:root:Rank 1 training batch 685 loss 0.21719776093959808
INFO:root:Rank 1 training batch 690 loss 0.2892182767391205
INFO:root:Rank 1 training batch 695 loss 0.5015683770179749
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 700 loss 0.15281304717063904
INFO:root:Rank 1 training batch 705 loss 0.2166418731212616
INFO:root:Rank 1 training batch 710 loss 0.12597529590129852
INFO:root:Rank 1 training batch 715 loss 0.33005836606025696
INFO:root:Rank 1 training batch 720 loss 0.3803814947605133
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 725 loss 0.11111056804656982
INFO:root:Rank 1 training batch 730 loss 0.20220468938350677
INFO:root:Rank 1 training batch 735 loss 0.2564409673213959
INFO:root:Rank 1 training batch 740 loss 0.32584694027900696
INFO:root:Rank 1 training batch 745 loss 0.27063947916030884
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 750 loss 0.22873994708061218
INFO:root:Rank 1 training batch 755 loss 0.16809837520122528
INFO:root:Rank 1 training batch 760 loss 0.3645368814468384
INFO:root:Rank 1 training batch 765 loss 0.22428955137729645
INFO:root:Rank 1 training batch 770 loss 0.6573272943496704
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 775 loss 0.11395681649446487
INFO:root:Rank 1 training batch 780 loss 0.1899539977312088
INFO:root:Rank 1 training batch 785 loss 0.178282231092453
INFO:root:Rank 1 training batch 790 loss 0.37838760018348694
INFO:root:Rank 1 training batch 795 loss 0.23684820532798767
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 800 loss 0.3227792978286743
INFO:root:Rank 1 training batch 805 loss 0.18677793443202972
INFO:root:Rank 1 training batch 810 loss 0.2992818355560303
INFO:root:Rank 1 training batch 815 loss 0.051499173045158386
INFO:root:Rank 1 training batch 820 loss 0.1780490130186081
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 825 loss 0.4496729373931885
INFO:root:Rank 1 training batch 830 loss 0.7619409561157227
INFO:root:Rank 1 training batch 835 loss 0.26574069261550903
INFO:root:Rank 1 training batch 840 loss 0.40412721037864685
INFO:root:Rank 1 training batch 845 loss 0.3981057107448578
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 850 loss 0.1181652694940567
INFO:root:Rank 1 training batch 855 loss 0.3168129324913025
INFO:root:Rank 1 training batch 860 loss 0.12688475847244263
INFO:root:Rank 1 training batch 865 loss 0.24623261392116547
INFO:root:Rank 1 training batch 870 loss 0.31441691517829895
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 875 loss 0.5721409320831299
INFO:root:Rank 1 training batch 880 loss 0.08651265501976013
INFO:root:Rank 1 training batch 885 loss 0.46506598591804504
INFO:root:Rank 1 training batch 890 loss 0.1820017695426941
INFO:root:Rank 1 training batch 895 loss 0.30698129534721375
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 900 loss 0.06675264239311218
INFO:root:Rank 1 training batch 905 loss 0.11940380185842514
INFO:root:Rank 1 training batch 910 loss 0.41682636737823486
INFO:root:Rank 1 training batch 915 loss 0.30498364567756653
INFO:root:Rank 1 training batch 920 loss 0.2652832567691803
INFO:root:Rank 1 averaging model across all trainers.
INFO:root:Rank 1 training batch 925 loss 0.29723960161209106
INFO:root:Rank 1 training batch 930 loss 0.11138555407524109
INFO:root:Rank 1 training batch 935 loss 0.31033408641815186
INFO:root:Training complete!
INFO:root:Getting accuracy....
INFO:root:Accuracy 0.9216
